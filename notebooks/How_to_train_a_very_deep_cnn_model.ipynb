{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How_to_train_a_very_deep_cnn_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAcnDUEFtlj1"
      },
      "source": [
        "# How to train a very deep CNN model?\n",
        "### Some major villains when training ‘very deep’ networks:\n",
        "* Vanishing Gradient\n",
        "* Exploding Gradient\n",
        "* Internal Covariate Shift\n",
        "\n",
        "### `ReLU` and `BatchNormalization` greatly resolve these\n",
        "* But, BatchNormalization slows down training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_RwAhqCbbO9"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYULkdoic8KM"
      },
      "source": [
        "( train_images, train_labels ), ( test_images, test_labels ) = mnist.load_data()\n",
        "print('Validation dataset:')\n",
        "print(test_images.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMQo41CSc-Om"
      },
      "source": [
        "plt.matshow( test_images[2], cmap = 'gray')\n",
        "plt.show()\n",
        "print(test_labels[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gzk_FmPdAZa"
      },
      "source": [
        "# Use the Test Set to do the training (because it is smaller)\n",
        "train_images = test_images.reshape( ( 10000, 28, 28, 1 ) )\n",
        "train_images = train_images.astype( 'float32' ) / 255\n",
        "print(\"Before:\")\n",
        "print( test_labels.shape )\n",
        "print( test_labels[0] )\n",
        "\n",
        "train_labels = to_categorical( test_labels )\n",
        "print(\"After:\")\n",
        "print( train_labels.shape )\n",
        "print( train_labels[0] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpNgVtPEgkoL"
      },
      "source": [
        "### I. A model with 4 CNN layers does reasonably well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLG0G3PsdDkm"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu', input_shape = (28,28,1)))\n",
        "model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu'))\n",
        "model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu'))\n",
        "model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 16, activation = 'relu'))\n",
        "model.add(Dense(units = 16, activation = 'relu'))\n",
        "model.add(Dense(units = 10, activation = 'softmax'))\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AbdIEs2dJqV"
      },
      "source": [
        "history = model.fit( train_images, train_labels, epochs = 4, batch_size = 10, validation_split = 0.2 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggGtujC9gou0"
      },
      "source": [
        "## II. A model with ~1000 CNN layers cannot learn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WYfK1ydZKU-"
      },
      "source": [
        "import sys\n",
        "print(sys.getrecursionlimit())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTB_P4mGfGIe"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu', input_shape = (28,28,1)))\n",
        "\n",
        "for i in range(800):\n",
        "    model.add(Conv2D(filters = 4, kernel_size = 3, activation = 'relu', padding='same'))\n",
        "\n",
        "model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 16, activation = 'relu'))\n",
        "model.add(Dense(units = 16, activation = 'relu'))\n",
        "model.add(Dense(units = 10, activation = 'softmax'))\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA1CbigdfT8T"
      },
      "source": [
        "history = model.fit( train_images, train_labels, epochs = 8, batch_size = 10, validation_split = 0.2 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efbg28xxgrRS"
      },
      "source": [
        "## III. A model with ~32 CNN layers also may not learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or0JF-jxf7Bi"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu', input_shape = (28,28,1)))\n",
        "\n",
        "for i in range(16):\n",
        "    model.add(Conv2D(filters = 4, kernel_size = 3, activation = 'relu', padding='same'))\n",
        "    model.add(Conv2D(filters = 4, kernel_size = 3, activation = 'relu', padding='same'))\n",
        "\n",
        "model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 16, activation = 'relu'))\n",
        "model.add(Dense(units = 16, activation = 'relu'))\n",
        "model.add(Dense(units = 10, activation = 'softmax'))\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZGVoT_DgDPI"
      },
      "source": [
        "history = model.fit( train_images, train_labels, epochs = 4, batch_size = 10, validation_split = 0.2 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyFa834Fgz-R"
      },
      "source": [
        "## IV. BatchNormalization comes to rescue a model with reasonable # of layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rbQeixbwL7b"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu', input_shape = (28,28,1)))\n",
        "\n",
        "for i in range(16):\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters = 4, kernel_size = 3, activation = 'relu', padding='same'))\n",
        "    model.add(Dropout(rate=0.4))\n",
        "    model.add(Conv2D(filters = 4, kernel_size = 3, activation = 'relu', padding='same'))\n",
        "\n",
        "model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 16, activation = 'relu'))\n",
        "model.add(Dense(units = 16, activation = 'relu'))\n",
        "model.add(Dense(units = 10, activation = 'softmax'))\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHAbcIHBwQrA"
      },
      "source": [
        "history = model.fit( train_images, train_labels, epochs = 8, batch_size = 10, validation_split = 0.2 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7CZ96eRfL8V"
      },
      "source": [
        "**Summary:**  \n",
        "A model with 4 CNN layers does reasonably well  \n",
        "A model with ~1000 CNN layers did not learn  \n",
        "A model with ~32 CNN layers also did not learn  \n",
        "BatchNormalization comes to rescue a model with reasonable # of layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsW5BB8TmQeV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}