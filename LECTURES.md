# Course topics

## 1. Introduction to Python and Numpy
1. [How to use Google Colab for Python programming?](https://www.youtube.com/watch?v=PVsS9WtwVB8)   
    Please note that I do not recommend upgrading to the Google Colab Pro (paid) version. All students who have paid for the "pro" version have shared that it is no better than the "free" version.  
1. [Python3](https://youtube.com/watch?v=V42qfAPybp8) / [notebook](notebooks/python.ipynb)
1. [Numpy](https://www.youtube.com/watch?v=Omz8P8n-5gY) / [notebook](notebooks/numpy.ipynb)
1. [Matplotlib & Plotly](https://youtu.be/aIzkkjRzVdA) / [notebook](notebooks/matplotlib_plotly.ipynb) 

**Optional:**
1. Practice Python at [codewars.org](https://www.codewars.com/)
1. [From Python to Numpy](https://www.labri.fr/perso/nrougier/from-python-to-numpy/)   
1. [100 numpy exercises](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises.ipynb) by Nicolas P. Rougier  
1. [Install Tensorflow in your local macine](https://www.youtube.com/watch?v=hHWkvEcDBO0&t)

## 2. Introduction to deep learning (Sections 1.1, 1.2, 1.3, and 4.1)
1. [Difference between AI, ML, and DL](https://youtu.be/kd62-4_jNoA)
1. [Introduction to deep learning](https://youtu.be/DGXuhXMgOO8)
1. [The power of a hidden layer in neural networks](https://youtu.be/V9x7SY_4y8c)
1. [How does machine learning (or deep learning) work? The intuition](https://youtu.be/Bp7zjKWRhAw)
1. [The four branches of machine learning](https://youtu.be/FlhcbzT2RUU)
1. [Learning 'bleeding-edge' deep learning](https://youtu.be/X8sCDMrPhAo)

**Optional:** [Deep Learning In 5 Minutes | What Is Deep Learning?](https://youtu.be/1k37OcjH7BM)

## 3. Data representations & tensor operations (Sections 2.2, 2.3, and 2.4) 
1. [What are tensors? Matrix vs Tensor](https://youtu.be/7FeO4lqcNfA)
1. [Tensors reshape automatically](https://youtu.be/92gOeXFq2FA)
1. [Examples of 3D, 4D, and 5D tensors](https://youtu.be/8gOg4VNRUaY)
1. [The gears of neural networks: Tensor operations](https://youtu.be/rv9w4MfnWgQ)
1. [Geometric interpretation of deep learning](https://youtu.be/h30cyYjXFIU)

**Optional:** [Lecture on TF2 by Joshwa Gordon @ Google](https://youtu.be/5ECD8J3dvDQ)

## 4. Introduction to Keras (Sections 3.2 and 3.3) 
1. [Introduction to Keras](https://youtu.be/Ym34JC2UDFk)
1. [Keras is also an API in Tensorflow2](https://youtu.be/yNsQ6rqEcv4)
1. [Keras sequential vs functional API](https://youtu.be/EvGS3VAsG4Y)
1. [Diversity of thought is holding back AI & deep learning research](https://youtu.be/pXMFMs1ryy4)
1. [AlphaFold2: Example of the power of diversity](https://youtu.be/gg7WjuFs8F4)
1. [Splitting data into development set + (training & validation) and test set + Callbacks](https://youtu.be/OeZ6i-8xXwQ)
1. [Binary classification using feed-forward neural networks](https://youtu.be/cJ3oqHqRBF0) / [notebook](./notebooks/wine_quality.ipynb) 

**Optional:** [Francois Chollet interview](https://youtu.be/Bo8MY4JpiXE)

## 5. Preparing images for deep learning (Sections 3.6.2, 5.2.4, and 5.2.5)
1. [Image is all numbers](https://youtu.be/mjh5NIn1yHk) (watch the first five minutes only)
1. [Data generators and image augmentation](https://youtu.be/dSs3kjqvv_Q) 
1. [Image preprocessing](https://youtu.be/9_OFSSYcVWU) / [notebook](./notebooks/Image_preprocessing.ipynb)

## 6. The convolution operation (Section 5.1.1) 
1. [Our eye and human visual system: Biological inspiration for convolutional neural networks](https://youtu.be/nu9Jdvhe1Pk)
1. [Our eyes have blind spots](https://youtu.be/QXzgokis33I) / [article](https://lasikofnv.com/try-these-3-fun-tests-to-find-your-visual-blind-spot/)
1. [Feed-forward (dense) vs Convolutional Neural networks](https://youtu.be/aU6lRpMkBkE)
1. [Hulk vs. Ant Man](https://youtu.be/fNGSHrQDuA8)
1. [The convolution operation](https://youtu.be/C73AemPLnL8)
1. [A convolutional neuron (filter): An example](https://youtu.be/oqf79zcafao)
1. [The two main parameters of a convolutional layer](https://youtu.be/GeBh1yd_H_E)
1. [How to calculate the number of parameters in a convolutional neural network? Some examples](https://youtu.be/bikmA-VmSbY)
1. [Border effect, padding, and maxpooling](https://youtu.be/MTmn--tHbHs)
1. [Separable convolutions and dilated convolutions](https://youtu.be/vCJ4magCPts)
1. A practical example: [What can one convolutional neuron do? Detect a square.](https://youtu.be/A69TFtiOREU) / [notebook](./notebooks/Detect_a_square.ipynb)
1. [Classify MNIST digits using a CNN](https://youtu.be/jd4-zRwYjDY) / [notebook](./notebooks/MNIST.ipynb)

**Reading:** [Intuitively Understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) / [alternative article](https://www.topbots.com/intuitively-understanding-convolutions-deep-learning/)

## 7. Activations & loss functions (Sections 4.5.5, and Table 4.1) 
1. [How to choose the last layerâ€™s activation and loss in NN?](https://youtu.be/veo6l7YkPhg)
1. [Softmax activation & other activations for deep neural networks](https://youtu.be/Q9p9cHo7rPk)
1. [Cross entropy loss (log loss) for binary classification](https://youtu.be/zhuuD9gckYo)
1. [Categorial cross-entropy loss (softmax loss) for multi-class classification](https://youtu.be/ILmANxT-12I)
1. [How to choose a loss function for a regression problem?](https://youtu.be/oWIYQNfm8tE)
1. [How to choose an optimizer for a Tensorflow Keras model?](https://youtu.be/pd3QLhx0Nm0)

## 8. Model evaluation, overfitting, underfitting, & regularization (Sections 4.2, 4.4, and 4.5) 
1. [The Blind Men and the Elephant](https://youtu.be/Vn9BUfUCL4I)
1. [Evaluating machine learning models: Measuring generalization](https://youtu.be/bjOX_aFXWQE)
1. [Overfitting (variance) and underfitting (bias)](https://youtu.be/sbYOtFV6aFc)
1. [How to prevent overfitting? Regularization techniques in deep learning](https://youtu.be/mbhXndDIzlc)
1. [L1 and L2 regularization](https://youtu.be/n-aOC_Q6WYs)
1. [Regularization using Dropout](https://youtu.be/BJYvbZhGArw)
1. [Regularization using Batch Normalization](https://youtu.be/sdXfAY_VD58)
1. [How to train deeper convolutional neural networks?](https://youtu.be/G6HA8yCpFAo) / [notebook](./notebooks/How_to_train_a_very_deep_cnn_model.ipynb) 
1. [Deep learning workflow/Recipe: From data to deep learning model](https://youtu.be/nPI0vK62B8g)
1. [How to debug a deep learning development pipeline?](https://youtu.be/cwvqche_eCY)

## 9. Classic CNN architectures (Sections 5.1.1, 5.1.2, and 7.1) 
<!--
1. [LeNet-5, AlexNet, and VGG-16 (17 min watch)](https://www.coursera.org/lecture/convolutional-neural-networks/classic-networks-MmYe2) (including the 'Advanced' content)
    Note: This lecture is **excluded** for the module quiz but the contents should be **included** in your concept map.
1. [Training your own LeNet-5](https://youtu.be/zwSXSltRhh0) / [notebook](./notebooks/LeNet.ipynb)
1. [Using VGG-16 (in Keras)](https://youtu.be/41d_yuANkb4) / [notebook](./notebooks/Using_VGG16.ipynb) 
1. [Effect of architecture depth on learning](https://youtu.be/4KEojbSJy4Y) / [notebook](./notebooks/How_to_train_a_very_deep_cnn_model.ipynb)
1. A) [Residual networks]
1. [Training very deep models with ResNet](https://youtu.be/SnkcHjH5tjI) / [notebook](./notebooks/Training_using_residual_connections.ipynb)
1. B) [Why do residual networks work?]
1. C) [The purpose of "1 by 1" convolutions]
3. Some state-of-the-art deep learning architectures for computer vision problems (Note: The following lectures are **excluded** for the module quiz but the contents should be **included** in your concept map.):
    - [Inception network](https://www.coursera.org/lecture/convolutional-neural-networks/inception-network-piR0x)
    - [DenseNet](https://youtu.be/-W6y8xnd--U)
    - [NASNet](https://youtu.be/sROrvtXnT7Q)
4. D) [How to choose pretrained models?]

- [slides](https://docs.google.com/presentation/d/1a5yeHRI_i0INatg9rLVpYuNTNvrxLCLxKH5_RISFwEY/edit?usp=sharing) / [notebooks](./notebooks/)
-->

## 10. Deep learning practices (Sections 4.3, 5.3, 5.4, and 7.1) 
<!--
1. [slides](https://docs.google.com/presentation/d/15qI0K9Sm4Ab1vp0x6fKyeCmweMZggTh237zfSxwj-B0/edit?usp=sharing)
1. [Feature engineering] - [slides](https://docs.google.com/presentation/d/14k2vUTlJThQ0u8RVc0C68_92K1Df5YW0v85C5w3nFe8/edit?usp=sharing) 
1. [Multi-input and Multi-output models]
1. [Layer weight sharing (The Siamese LSTM)]
1. GPUs for deep learning - [slides](https://docs.google.com/presentation/d/1Jg-BOZBDfhBht_3Sf49ja8QrWK_QuX7pr1CQkAf2mcI/edit?usp=sharing)
1. Transfer learning - [slides](https://docs.google.com/presentation/d/1OV2KDijNYVnwYUrpp0otFCGyt-mejSsvtArp3UyrMQM/edit?usp=sharing) / [notebook](./notebooks/Transfer_learning.ipynb)
1. [What is Explainable AI (XAI)?](https://vimeo.com/278690594)
1. [Techniques for interpreting a deep learning model]
-->
**Reading:** [Neural Network Follies](https://neil.fraser.name/writing/tank/) [Note: This must be included in your concept map]

## 11. Potential and Limitations of deep learning (Section 9.2)
1. [Goals of deep learning](https://youtu.be/HJ1RX0jowhM)
1. [Limitations of deep learning](https://youtu.be/X5PQEEwCeKg)
2. Read: [One of biology's biggest mysteries 'largely solved' by AI](https://www.bbc.com/news/science-environment-55133972)

**Optional:** Resources for implementing the backpropagation algorithm:
- [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
- [CSE 599G1: Deep Learning System, University of Washington](https://dlsys.cs.washington.edu/pdf/lecture4.pdf) 
- [Backpropagation, NNets](https://www.youtube.com/watch?v=i94OvYb6noo)

**Optional:** [Andrew Ng: Advice on Getting Started in Deep Learning](https://youtu.be/1k37OcjH7BM)
